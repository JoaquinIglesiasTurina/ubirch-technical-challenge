* Introduction
This repo provides an answer for the Ubirch backend technical challenge.
I'll let you decide whether that answer is correct.

* Running the code
=Docker= and =sbt= are required to run the code.
All commands here are run on the project's root directory.

There is a =docker-compose.yml= file on the root directory.
It handles the containerized =Kafka= instance, which can be instantiated
by running the following command.
#+begin_src bash
  docker-compose up
#+end_src

The following command actually runs the code.
You might need to run it on a different terminal than the previous one,
or do some ampersand shenanigans.
#+begin_src bash
  sbt run
#+end_src
And then you always have the option of running the code
through your favorite IDE.

* Answering the challenge questions

*** What assumptions do you have to make? if any.
The largest assumption of this answer is that the problem can be solved using an
event-driven / conveyor-belt architecture.

This means the possibility of getting a callback to start the chain of events exists.

*** How would you solve this challenge?
I would not. I would exploit that question number 7 was asked.
Then proceed to solve a simpler problem, where I have more expertise
and allows me to tinker with Kafka and Event Driven Architecture. ;)

*** How would you classify this problem?
There are several instances polling and tyring to modify a centralized database.
This is a distributed coordination problem. Or [[https://medium.com/@lalitadithya/everything-ive-learnt-about-distributed-locking-so-far-1f1569e6df5][so]] I've [[https://old.reddit.com/r/microservices/comments/9e2wzb/avoid_duplicate_work_while_scaling_microservices/][read on]] the [[https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html][Internet]].

*** What technique would you apply in order to accomplish this goal?
A distributed lock.

*** What technologies would you suggest to use?
Kafka, [[https://zookeeper.apache.org/doc/current/recipes.html#sc_recipes_Locks][Zookeeper]] and Redis have facilities to model this kind of problem.
Based on the [[https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html][cited sources]], Zookeeper has the better set of features,
allowing for fencing and time based consensus solving.

But yet again, my suggestion is to avoid global state management,
and use Kafka with an event-driven architecture.

*** How would you model it based on the current view?
- An object handling each of the processes business logic.
- A piece of global logic, handling the case that a particular resource is locked.
  If an instance is trying to access a locked resource for process =A=, it may
  be worth while to cancel the attempt and what for the next schedule to fire.

*** Could you suggest an alternative model view as presented above to fix this issue?
An event-driven / conveyor-belt architecture may solve this issue is a simpler way.
Rather than polling the data-base and using a distributed lock service to coordinate instances,
We could use a messaging service to communicate the state of different processes between instances.

Under this approach, the code running on each instance is stateless, there is no need for 
distributed lock logic nor for any other global state.
This should make code more testable and easier to reason about, is =reasonable= a word in English?

*** Could you suggest a way to test the data consistency?
Merkle trees could be used to provide cryptographical guarantees for data consistency.
Each step of the computation could be modeled as a node of the tree, and thus, cryptographically audited.

I've learnt about [[https://github.com/ubirch/ubirch-event-log][this]] while scouring Github for a cheat on this challenge.
I hope there is bonus points for honesty.
** Could you write a simple program were external tools are mocked, you use tasks schedulers to model the background processes and put it on Github.
You are looking at it.

* Code walk through
Each event plus the =Kafka Client= configuration and =Main= app have been modeled into their own
objects. For a codebase this size, this may be overkill (it was a later refactor of the code).
But it ensures each code snippet here is correct and should compile.

** Imports
#+begin_src scala
  import org.apache.kafka.clients.producer.{ProducerRecord, RecordMetadata}
  import zio._
  import zio.Console.printLine
  import zio.Schedule.WithState
  import zio.kafka.consumer.{Consumer, ConsumerSettings, Subscription}
  import zio.kafka.producer.{Producer, ProducerSettings}
  import zio.kafka.serde.Serde
  import zio.stream._

  import java.util.UUID
#+end_src

** Kafka Configuration
#+begin_src scala
  object KafkaClient {
    private val producerSettings = ProducerSettings(List("localhost:9092"))
    private val producerResource = Producer.make(producerSettings)
    val producer: ZLayer[Any, Throwable, Producer] = ZLayer.scoped(producerResource)

    private val consumerSettings = ConsumerSettings(List("localhost:9092"))
      .withGroupId("updates-consumer")
    private val managedConsumer = Consumer.make(consumerSettings) //efectful resources
    val consumer: ZLayer[Any, Throwable, Consumer] = ZLayer.scoped(managedConsumer) // effectful DI
  }
#+end_src

** MintEvent: The triggering event
This is the event that triggers the whole pipeline.
The object contains a method producing a new record into =Kafka=.
Then we have the event's logic:
- The call to =Random.nextUUID= simulates accessing an external service,
  where the state of the minting process is checked.
- =Console.printLine= is a side effect, maybe updating something in db.
- The record of the event is sent to =Kafka=
The last field of the object is the schedule on which it runs.
#+begin_src scala
  object MintEvent {
    private def produceMintRecord(uuid: UUID): RIO[Producer, RecordMetadata] = {
      val record = new ProducerRecord[UUID, String]("mints", uuid, "minted")
      Producer.produce[Any, UUID, String](record, Serde.uuid, Serde.string)
    }
    val getNewMint: ZIO[Producer, Throwable, UUID] = for {
      uuid <- Random.nextUUID
      _ <- printLine(s"minted item with uuid: $uuid")
      _ <- produceMintRecord(uuid)
    } yield uuid
    val mintSchedule: WithState[Long, Any, Any, Long] = Schedule.spaced(3.seconds)
  }
#+end_src

** FindUsers: An intermediate step
This object simulates an intermediate step, it consumes messages produced by the previous event.
Then some new messages are produced.
- The =users= =List= is the fake database.
- Again, the call to =Random= simulates accessing an external service,
  to find which users care for a particular minted item.
- and =printLine= is used to simulate some side effect.

#+begin_src scala
  object FindUsers {
    private val users = List.fill(15)(java.util.UUID.randomUUID())

    private def produceUserCallback(userID: UUID, itemID: UUID): RIO[Producer, RecordMetadata] = {
      val record = new ProducerRecord[UUID, UUID]("usercallback", userID, itemID)
      Producer.produce[Any, UUID, UUID](record, Serde.uuid, Serde.uuid)
    }

    private def getUsersOfMinted(key: UUID, value: String): ZIO[Producer, Throwable, Unit] = {
      ZStream.fromIterableZIO {
        for {
          u0 <- Random.shuffle(users)
        } yield u0.take(3)
      }.tap { user => produceUserCallback(user, key) }
        .tap { user => printLine(s"user with id $user is interested on item $key") }
        .run(ZSink.drain)
    }

    private val mintsConsumer = Consumer.subscribeAnd(Subscription.topics("mints"))
      .plainStream(Serde.uuid, Serde.string)
    private val mintStream = mintsConsumer.map(cr => (cr.key, cr.value, cr.offset))
      .tap { case (key, value, _) => getUsersOfMinted(key, value) }
      .map(_._3).aggregateAsync(Consumer.offsetBatches)
    val mintStreamEffect: ZIO[Producer with Any with Consumer, Throwable, Unit] =
      mintStream.run(ZSink.foreach(record => record.commit))
  }
#+end_src

** CallbackNotificantion: The leaf case
A simpler version of the previous case. Just consuming and side effecting.
#+begin_src scala
  object CallbackNotification {
    private val callbacksConsumer = Consumer.subscribeAnd(Subscription.topics("usercallback"))
      .plainStream(Serde.uuid, Serde.uuid)

    private val callbacksStream = callbacksConsumer.map(cr => (cr.key, cr.value, cr.offset))
      .tap { case (key, value, _) =>
        printLine(s"sending email notification to user with id $key about item with id $value")
      }
      .map(_._3).aggregateAsync(Consumer.offsetBatches)
    val callbacksEffect: ZIO[Any with Consumer, Throwable, Unit] =
      callbacksStream.run(ZSink.foreach(record => record.commit))
  }
#+end_src
** Main. The app proper
Running the previous object's effects.
#+begin_src scala
  object Main extends ZIOAppDefault {
    override def run: ZIO[Environment with ZIOAppArgs with Scope, Any, Any] = {
      for {
        _ <- CallbackNotification.callbacksEffect.provideSomeLayer(KafkaClient.consumer).fork
        _ <- FindUsers.mintStreamEffect.provideSomeLayer(KafkaClient.consumer ++ KafkaClient.producer).fork
        _ <- MintEvent.getNewMint.provideSomeLayer(KafkaClient.producer) repeat MintEvent.mintSchedule
      } yield ()
    }
  }
#+end_src
